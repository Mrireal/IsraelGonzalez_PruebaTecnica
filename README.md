# Sistema Multiagente RAG con Gemini (Prueba TÃ©cnica)

Este repositorio contiene la soluciÃ³n a la prueba tÃ©cnica para implementar un sistema inteligente de consulta sobre protocolos escolares ("Ejes del RICE").

El sistema utiliza **LangGraph** para orquestar un flujo de trabajo multiagente, **Gemini 2.0 Flash (Vertex AI)** como motor de razonamiento, y **FastAPI** para exponer el servicio.

## ğŸš€ CaracterÃ­sticas Principales

*   **Arquitectura Multiagente:** Uso de LangGraph para coordinar decisiones complejas.
    *   **Router Agent:** Clasifica la intenciÃ³n del usuario (Â¿Pregunta sobre el colegio o charla general?).
    *   **RAG Agent:** Recupera informaciÃ³n precisa de documentos locales (PDFs).
    *   **Answer Agent:** Sintetiza la respuesta final manteniendo el tono y las reglas del negocio.
*   **RAG (Retrieval-Augmented Generation) Local:**
    *   Carga automÃ¡tica de documentos desde la carpeta `kb/`.
    *   Embeddings locales (`sentence-transformers/all-MiniLM-L6-v2`) para eficiencia y ahorro de cuotas.
    *   Base de datos vectorial **FAISS** para bÃºsquedas semÃ¡nticas rÃ¡pidas.
*   **Backend Robusto:**
    *   API REST con **FastAPI**.
    *   ValidaciÃ³n de datos con **Pydantic**.
    *   Manejo de errores y logs.
*   **ContenerizaciÃ³n:** Listo para despliegue con **Docker**.

---

## ğŸ“‚ Estructura del Proyecto

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents.py       # LÃ³gica de los agentes y el grafo de LangGraph
â”‚   â”œâ”€â”€ config.py       # ConfiguraciÃ³n centralizada y variables de entorno
â”‚   â”œâ”€â”€ main.py         # Entry point de la API (FastAPI)
â”‚   â”œâ”€â”€ models.py       # Modelos de datos Pydantic (Request/Response)
â”‚   â”œâ”€â”€ rag.py          # Servicio RAG: Carga de docs, embeddings y FAISS
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ faiss_index/        # Almacenamiento persistente del Ã­ndice vectorial
â”œâ”€â”€ kb/                 # Base de conocimiento (PDFs, TXT, MD)
â”œâ”€â”€ .env                # Variables de entorno (Credenciales)
â”œâ”€â”€ Dockerfile          # DefiniciÃ³n de la imagen Docker
â”œâ”€â”€ interactive_chat.py # Script para probar el agente desde la terminal
â”œâ”€â”€ requirements.txt    # Dependencias del proyecto
â””â”€â”€ README.md           # DocumentaciÃ³n
```

---

## ğŸ› ï¸ InstalaciÃ³n y Uso Local

### 1. Prerrequisitos
*   Python 3.10+
*   Cuenta de Google Cloud Platform (GCP) con Vertex AI habilitado.
*   Archivo JSON de credenciales de Service Account.

### 2. ConfiguraciÃ³n
1.  Clona este repositorio.
2.  Crea un entorno virtual:
    ```bash
    python -m venv .venv
    .\.venv\Scripts\Activate  # Windows
    source .venv/bin/activate # Linux/Mac
    ```
3.  Instala las dependencias:
    ```bash
    pip install -r requirements.txt
    ```
4.  Configura las variables de entorno:
    *   Crea un archivo `.env` en la raÃ­z.
    *   Agrega la ruta a tu archivo de credenciales de Google:
        ```env
        GOOGLE_APPLICATION_CREDENTIALS=tu-archivo-credenciales.json
        ```
5.  Coloca tus documentos PDF en la carpeta `kb/`.

### 3. EjecuciÃ³n
Inicia el servidor de desarrollo:
```bash
uvicorn app.main:app --reload
```
La API estarÃ¡ disponible en `http://127.0.0.1:8000`.(si da algun error de librerias por ejemplo no existe langchain-google-vertexai, es necesario hacerlo de la siguiente manera: 1.- .\.venv\Scripts\Activate 2.- uvicorn app.main:app --reload)

---

## ğŸ³ EjecuciÃ³n con Docker

1.  **Construir la imagen:**
    ```bash
    docker build -t rice-agent .
    ```

2.  **Ejecutar el contenedor:**
    ```bash
    docker run -p 8000:8000 --env-file .env rice-agent
    ```

---

## ğŸ§ª Pruebas

### Chat Interactivo (Terminal)
He incluido un script para probar el agente fÃ¡cilmente sin herramientas externas:
```bash
python interactive_chat.py
```

### Swagger UI
Visita `http://127.0.0.1:8000/docs` para interactuar con la API visualmente.

### Ejemplo de PeticiÃ³n (cURL)
```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/query' \
  -H 'Content-Type: application/json' \
  -d '{
  "question": "Â¿QuÃ© es el RICE?"
}'
```

---

## ğŸ§  Decisiones TÃ©cnicas

*   **Embeddings Locales vs API:** Se optÃ³ por usar `HuggingFaceEmbeddings` localmente en lugar de los embeddings de Vertex AI. Esto reduce la latencia de red para la vectorizaciÃ³n y evita el consumo innecesario de cuotas de API durante la indexaciÃ³n.
*   **LangGraph:** Se eligiÃ³ sobre cadenas secuenciales simples para permitir flujos condicionales mÃ¡s complejos en el futuro (ej: bucles de correcciÃ³n si la respuesta no es satisfactoria).
*   **Gemini 2.0 Flash:** Seleccionado por su balance entre velocidad, costo y capacidad de razonamiento.

---
**Autor:** Israel Gonzalez (Postulante)
